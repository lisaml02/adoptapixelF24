---
title: "eda"
output: html_document
date: "2024-09-29"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r loading packages and data}
library(tidyverse)
library(dplyr)

globe <- read_csv("./GLOBE_data.csv")
```

# Cleaning

```{r rename columns}
globe <- globe %>% 
  rename(landcover_id = `land covers:land cover id`) %>% 
  rename(data_source = `land covers:data source`) %>% 
  rename(measured_at = `land covers:measured at`) %>% 
  rename(muc_code = `land covers:muc code`) %>% 
  rename(muc_description = `land covers:muc description`) %>% 
  rename(north_photo_url = `land covers:north photo url`) %>% 
  rename(east_photo_url = `land covers:east photo url`) %>% 
  rename(south_photo_url = `land covers:south photo url`) %>% 
  rename(west_photo_url = `land covers:west photo url`) %>% 
  rename(upward_photo_url = `land covers:upward photo url`) %>%
  rename(downward_photo_url = `land covers:downward photo url`) %>%
  rename(measure_lat = `land covers:measurement latitude`) %>% 
  rename(measure_long = `land covers:measurement longitude`) %>% 
  rename(measure_elev = `land covers:measurement elevation`) %>% 
  rename(loc_method = `land covers:location method`) %>% 
  rename(loc_accuracy = `land covers:location accuracy (m)`) %>% 
  rename(snow_ice = `land covers:snow ice`) %>% 
  rename(standing_water = `land covers:standing water`) %>% 
  rename(muddy = `land covers:muddy`) %>% 
  rename(dry_ground = `land covers:dry ground`) %>% 
  rename(leaves_on_trees = `land covers:leaves on trees`) %>% 
  rename(raining_snowing = `land covers:raining snowing`)
```

```{r select columns and format}
library(dplyr)

globe_c <- globe %>%
  select(
    organization_id, org_name, site_id, site_name, latitude, longitude, elevation, measured_on, 
    landcover_id, data_source, measured_at, muc_code, muc_description, north_photo_url, 
    east_photo_url, south_photo_url, west_photo_url, upward_photo_url, downward_photo_url, 
    measure_lat, measure_long, measure_elev, loc_method, loc_accuracy, snow_ice, standing_water, 
    muddy, dry_ground, leaves_on_trees, raining_snowing
  ) %>%
  mutate(
    snow_ice = ifelse(!is.na(snow_ice), 0, 1),
    standing_water = ifelse(!is.na(standing_water), 0, 1),
    muddy = ifelse(!is.na(muddy), 0, 1),
    dry_ground = ifelse(!is.na(dry_ground), 0, 1),
    leaves_on_trees = ifelse(!is.na(leaves_on_trees), 0, 1),
    raining_snowing = ifelse(!is.na(raining_snowing), 0, 1)
  ) %>%
  filter(if_all(everything(), ~ !is.na(.)))

globe_c

```

```{r correctly formatting columns and rounding decimal points}
globe_c$measured_on <- as.POSIXct(as.character(globe_c$measured_on), format = "%Y-%m-%d")

globe_c <- globe_c %>%
  mutate(
    organization_id = as.numeric(organization_id),
    site_id = as.numeric(site_id),
    latitude = round(as.numeric(latitude), 6),
    longitude = round(as.numeric(longitude), 6),
    elevation = round(as.numeric(elevation), 6),
    measure_lat = round(as.numeric(measure_lat), 6),
    measure_long = round(as.numeric(measure_long), 6),
    measure_elev = round(as.numeric(measure_elev), 6)
  )
globe_c
```

# Metadata Info

```{r}
# install.packages("cld2")
library(cld2)

# Check languages in the text_column of globe_c
language_counts <- table(cld2::detect_language(globe_c$org_name, plain_text = TRUE))

# Display the language counts
print(language_counts)

```

# Date/Time/Location

```{r}
min(globe_c$measured_at)
max(globe_c$measured_at)

min(globe_c$latitude)
min(globe_c$longitude)
min(globe_c$measure_lat)
min(globe_c$measure_long)

max(globe_c$latitude)
max(globe_c$longitude)
max(globe_c$measure_lat)
max(globe_c$measure_long)
```

```{r checking if measured_at = measured_on}
library(dplyr)

globe_c <- globe_c %>%
  mutate(
    are_equal = measured_at == measured_on
  )

# Display rows where the columns are not equal
globe_c_not_equal <- globe_c %>% filter(are_equal)
globe_c_not_equal
```

Date range where they are equal: 2018-09-30 to 2019-07-18

```{r calculating local time}
library(dplyr)
library(lubridate)

globe_c <- globe_c %>%
  mutate(
    # Calculate the time zone offset (round to the nearest hour)
    timezone_offset = round(measure_long / 15),
    
    # Calculate the local time by adding the offset to measured_on
    local_time = measured_on + hours(timezone_offset)
  )

globe_c
```

```{r converting location accuracy column into standardized units}
library(dplyr)

# Convert loc_accuracy from meters to degrees of latitude and longitude
globe_c <- globe_c %>%
  mutate(
    # Convert latitude offset: distance in meters divided by meters per degree of latitude
    latitude_offset_deg = loc_accuracy / 111320,
    
    # Convert longitude offset: accounts for latitude's effect on longitude distance
    longitude_offset_deg = loc_accuracy / (111320 * cos(measure_lat * pi / 180))
  )

globe_c
```

```{r site_ids with 10 observations or 1 observations}

# Assuming globe_c is your dataframe
site_id_counts <- globe_c %>%
  group_by(site_id) %>%
  summarise(count = n())

# Find site IDs with exactly 10 observations
site_ids_with_10_obs <- site_id_counts %>%
  filter(count == 10)

# Find site IDs with exactly 1 observation
site_ids_with_1_obs <- site_id_counts %>%
  filter(count == 1)

# Display results
site_ids_with_10_obs
site_ids_with_1_obs

```

```{r aggregating lat and long}
library(dplyr)

# Calculate the average of the latitude and longitude columns
globe_aggregated <- globe_c %>%
  summarise(
    avg_latitude = mean(latitude, na.rm = TRUE),
    avg_longitude = mean(longitude, na.rm = TRUE),
    avg_measure_lat = mean(measure_lat, na.rm = TRUE),
    avg_measure_long = mean(measure_long, na.rm = TRUE)
  )

# Display the results
globe_aggregated

```

```{r aggregating by degree blocks}
library(dplyr)

# Aggregate by degree blocks (round to the nearest whole number)
globe_block_aggregated <- globe_c %>%
  mutate(
    lat_block = floor(latitude),          # Find the latitude degree block
    long_block = floor(longitude),        # Find the longitude degree block
    measure_lat_block = floor(measure_lat),  # Measured latitude block
    measure_long_block = floor(measure_long) # Measured longitude block
  ) %>%
  group_by(lat_block, long_block) %>%  # Group by the original lat/long blocks
  summarise(
    avg_latitude = mean(latitude, na.rm = TRUE),
    avg_longitude = mean(longitude, na.rm = TRUE),
    avg_measure_lat = mean(measure_lat, na.rm = TRUE),
    avg_measure_long = mean(measure_long, na.rm = TRUE),
    count = n()  # Count of observations in each block
  )

# Display the results
globe_block_aggregated


library(dplyr)

# Count unique latitude blocks and their frequencies
lat_block_counts <- globe_block_aggregated %>%
  count(lat_block, name = "frequency_lat_block")

# Count unique longitude blocks and their frequencies
long_block_counts <- globe_block_aggregated %>%
  count(long_block, name = "frequency_long_block")

# Display the total number of unique latitude and longitude blocks
num_lat_blocks <- n_distinct(globe_block_aggregated$lat_block)
num_long_blocks <- n_distinct(globe_block_aggregated$long_block)

print(paste("Total unique latitude blocks:", num_lat_blocks))
print(paste("Total unique longitude blocks:", num_long_blocks))

# Display the frequency tables for latitude and longitude blocks
lat_block_counts
long_block_counts

```

```{r adding season column}
library(dplyr)
library(lubridate)

# Add the season column based on the local_time
globe_c <- globe_c %>%
  mutate(
    # Extract the month and day for classification
    month_day = format(local_time, "%m-%d"),
    
    # Determine the season based on the local_time date
    season = case_when(
      month_day >= "03-20" & month_day <= "06-20" ~ "Spring",
      month_day >= "06-21" & month_day <= "09-22" ~ "Summer",
      month_day >= "09-23" & month_day <= "12-20" ~ "Fall",
      TRUE ~ "Winter"  # Remaining dates are considered Winter
    )
  )

# View the updated dataset with the season column
head(globe_c)

```

```{r day of year}
library(dplyr)
library(lubridate)

# Add a column that represents the day of the year
globe_c <- globe_c %>%
  mutate(
    day_of_year = yday(local_time)  # yday() accounts for leap years
  )

# View the updated dataset with the day_of_year column
head(globe_c)

```

```{r}
write_csv(globe_c, "globe_cv2.csv")
```

## Outlier Analyses

```{r finding lat values that fall outside -90-90}
library(dplyr)

# Find rows where latitude or measured latitude falls outside the -90 to 90 range
invalid_latitudes <- globe_c %>%
  filter(latitude < -90 | latitude > 90 | measure_lat < -90 | measure_lat > 90)

# Display the rows with invalid latitude values
invalid_latitudes

```

```{r outlier analysis for location}
library(dplyr)

# Function to identify outliers using the IQR method
find_outliers <- function(x) {
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  IQR_value <- IQR(x, na.rm = TRUE)
  lower_bound <- Q1 - 1.5 * IQR_value
  upper_bound <- Q3 + 1.5 * IQR_value
  return(x < lower_bound | x > upper_bound)
}

# Apply outlier analysis to latitude and longitude columns
globe_c_outliers <- globe_c %>%
  mutate(
    lat_outlier = find_outliers(latitude),
    long_outlier = find_outliers(longitude),
    measure_lat_outlier = find_outliers(measure_lat),
    measure_long_outlier = find_outliers(measure_long)
  )

# Display rows where any of the latitude or longitude values are outliers
globe_c_outliers %>%
  filter(lat_outlier | long_outlier | measure_lat_outlier | measure_long_outlier)

```

```{r}
library(dplyr)

# Function to identify outliers using the IQR method
find_date_outliers <- function(day_of_year) {
  Q1 <- quantile(day_of_year, 0.25, na.rm = TRUE)
  Q3 <- quantile(day_of_year, 0.75, na.rm = TRUE)
  IQR_value <- IQR(day_of_year, na.rm = TRUE)
  lower_bound <- Q1 - 1.5 * IQR_value
  upper_bound <- Q3 + 1.5 * IQR_value
  return(day_of_year < lower_bound | day_of_year > upper_bound)
}

# Apply the outlier analysis to the day_of_year column
globe_c <- globe_c %>%
  mutate(
    date_outlier = find_date_outliers(day_of_year)
  )

# Display rows where the date is considered an outlier
date_outliers <- globe_c %>%
  filter(date_outlier)

# View the date outliers
date_outliers

```

```{r}
library(dplyr)

# Calculate the IQR bounds for the loc_accuracy column
loc_accuracy_stats <- globe_c %>%
  summarise(
    Q1 = quantile(loc_accuracy, 0.25, na.rm = TRUE),
    Q3 = quantile(loc_accuracy, 0.75, na.rm = TRUE),
    IQR_value = IQR(loc_accuracy, na.rm = TRUE)
  )

# Calculate the lower and upper bounds for outliers
lower_bound <- loc_accuracy_stats$Q1 - 1.5 * loc_accuracy_stats$IQR_value
upper_bound <- loc_accuracy_stats$Q3 + 1.5 * loc_accuracy_stats$IQR_value

# Identify outliers in the loc_accuracy column
globe_c <- globe_c %>%
  mutate(
    loc_accuracy_outlier = loc_accuracy < lower_bound | loc_accuracy > upper_bound
  )

# Display rows where loc_accuracy is an outlier
loc_accuracy_outliers <- globe_c %>%
  filter(loc_accuracy_outlier)

# View the outliers
loc_accuracy_outliers

```

## Graphs:

```{r graphing lat and long}
library(ggplot2)

# Assuming 'globe_c' is your dataframe
ggplot(globe_c) +
  # Plot measured latitude and longitude
  geom_point(aes(x = measure_long, y = measure_lat), color = 'blue', alpha = 0.6, size = 2) +
  # Plot latitude and longitude
  geom_point(aes(x = longitude, y = latitude), color = 'red', alpha = 0.6, size = 2, shape = 4) +
  labs(
    title = 'Latitude and Longitude Scatter Plot',
    x = 'Longitude',
    y = 'Latitude'
  ) +
  theme_minimal() +
  scale_color_manual(
    name = 'Coordinates',
    values = c('blue' = 'Measured Coordinates', 'red' = 'Original Coordinates')
  ) +
  theme(legend.position = 'top')

```

```{r distribution of time}
library(ggplot2)
library(lubridate)
library(dplyr)

# Extract hour, month, and year from local_time
globe_c <- globe_c %>%
  mutate(
    hour_of_day = hour(local_time),
    month_of_year = month(local_time, label = TRUE, abbr = TRUE),  # Label with month names
    year = year(local_time)
  )

# Plot 1: Distribution of Hour of the Day
plot_hour <- ggplot(globe_c, aes(x = hour_of_day)) +
  geom_histogram(binwidth = 1, fill = 'blue', color = 'black', alpha = 0.7) +
  labs(
    title = 'Distribution of Time (Hour of Day)',
    x = 'Hour of Day',
    y = 'Frequency'
  ) +
  theme_minimal()

# Plot 2: Distribution of Month of the Year
plot_month <- ggplot(globe_c, aes(x = month_of_year)) +
  geom_bar(fill = 'green', color = 'black', alpha = 0.7) +
  labs(
    title = 'Distribution of Time (Month of Year)',
    x = 'Month of Year',
    y = 'Frequency'
  ) +
  theme_minimal()

# Plot 3: Distribution of Years
plot_year <- ggplot(globe_c, aes(x = year)) +
  geom_bar(fill = 'purple', color = 'black', alpha = 0.7) +
  labs(
    title = 'Distribution of Time (Years)',
    x = 'Year',
    y = 'Frequency'
  ) +
  theme_minimal()

# Display the plots
plot_hour
plot_month
plot_year

```

```{r location accuracy distribution with outliers}
library(ggplot2)

# Boxplot for location accuracy
ggplot(globe_c, aes(x = loc_accuracy)) +
  geom_boxplot(fill = 'skyblue', color = 'black', outlier.colour = 'red', outlier.shape = 16, outlier.size = 2) +
  labs(
    title = 'Boxplot of Location Accuracy',
    x = 'Location Accuracy (meters)'
  ) +
  theme_minimal()

```

```{r accuracy distribution without outliers}
library(ggplot2)
library(dplyr)

# Calculate the IQR bounds for loc_accuracy
loc_accuracy_stats <- globe_c %>%
  summarise(
    Q1 = quantile(loc_accuracy, 0.25, na.rm = TRUE),
    Q3 = quantile(loc_accuracy, 0.75, na.rm = TRUE),
    IQR_value = IQR(loc_accuracy, na.rm = TRUE)
  )

# Calculate the lower and upper bounds for outliers
lower_bound <- loc_accuracy_stats$Q1 - 1.5 * loc_accuracy_stats$IQR_value
upper_bound <- loc_accuracy_stats$Q3 + 1.5 * loc_accuracy_stats$IQR_value

# Remove outliers from the data
globe_c_filtered <- globe_c %>%
  filter(loc_accuracy >= lower_bound & loc_accuracy <= upper_bound)

# Plot the boxplot without outliers
ggplot(globe_c_filtered, aes(x = loc_accuracy)) +
  geom_boxplot(fill = 'skyblue', color = 'black') +
  labs(
    title = 'Boxplot of Location Accuracy (Outliers Removed)',
    x = 'Location Accuracy (meters)'
  ) +
  theme_minimal()

```

# Notes:

check if measured at and measured on are equivalent

what is the date range where they are equal

measured at is reported from mobile device –\> reference to Greenwich mean time (could be 8 hours from when someone actually took it)

calculating local time could be useful

what to do with lat/long/elev

the only way to calculate local time is using lat/long at that date –\> ex. sun angle in Alaska in september vs december is different

remember: just because you can calculate it doesn't mean it has value

we should always have six digits in the lat/long measurement, even when rounded

time measurement should not go beyond seconds

we do have a bit of false accuracy –\> call people out on this in the future

each one of the decimal points in the lat/long measurement tells us more specifics of where we are on earth

measured lat and long actually come from the devices –\> start with these

they come from location services from a mobile device

we don't necessarily believe it until we find more info

if its an iphone, it will only get to 5 m within the coords bc of privacy reasons –\> androids go to 3m

location accuracy column: lat/long exist in degrees which are angular units, the accuracy field is in a distance measure (meters) so technically they don't really make sense but there are mathematical conversions are out there

for our purposes, we want to map the accuracy distribution –\> dont need to get rid of ones w a high value, we can interrogate them more –\> sort of like a measure of error

we can graph this, which is technically what a map is (lat = x and long = y)

get mins and maxs of lats and longs

nonmeasured lats and longs:

we have a global grid

we have cartesian coords that we put over places on the earth –\> origins have to be in diff places because the earth is not flat

the lat and long is in reference to that global grid –\> it is the south-west corner of the square

this is a way of grouping close things together

site-id : things are grouped together based on location and time

we are interested in location over time

find a count of which ones have 10 obs for that location? which ones have only 1? 1 isnt really as useful to us, but that's where most of it will be

at what level do we wanna agregate? essentially we wanna go back to the whole number (whats within the 45/46 degree lat range and the 129-130 long range) –\> this is called a degree block

look for lat measurements that fall outside -90 to 90 range –\> 0 is equator

if we average that range of values, are we getting participants north or south of the equator?

for the photos: what is the megabite size vs the image resolution? calculate this and plot it along a timeline

examine size more

order by lat/long

think about collaging these images into one image first –\> this emphasizes them

can use small.jpg 255 x 255

thumb.jpg is 64 x 64 –\> mosaic them into a 255 or 148 and then send that into a classifier

get distribution of time

grouping together based on seasons

calculate day of year: what day of year ? –\> ex. what day of the year is december 3rd of some year? like out of 365/366? what is the frequency of when we're getting our observations
