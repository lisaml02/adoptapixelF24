---
title: "analysis_nov17"
output: html_document
date: "2024-11-17"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Outlier Analysis

```{r loading packages}
library(tidyverse)
library(dplyr)
library(ggplot2)
```

```{r saving digital attributes }
# saving digital attributes with n = 100
# write_csv(results_df, "dig_att_100.csv")
results_df <- read_csv("dig_att_100.csv")
```

```{r separating digital attributes for analysis}
results_spectral <- results_df %>% 
  select(image_url, brightness, red_mean, green_mean, blue_mean, snow_ice, leaves_on_trees, GCC, BCC, RCC, color_magnitude)

head(results_spectral)
```

```{r outlier analysis}
outlier_df <- results_spectral %>% 
  {
    z_scores <- scale(select(., -image_url)) # Calculate Z-scores for numeric columns
    outliers <- apply(z_scores, 1, function(row) any(abs(row) > 3)) # Identify outliers
    .[outliers, ] # Return rows identified as outliers
  }

# Display outliers, including the image URLs
outlier_df

```

```{r outlier boxplots}
# Create boxplots for each numeric attribute using aes() and tidy evaluation
numeric_columns <- names(results_spectral)[!names(results_spectral) %in% "image_url"]

# Loop through each numeric column and create boxplots
for (col in numeric_columns) {
  p <- ggplot(results_spectral, aes(y = .data[[col]])) +
    geom_boxplot() +
    labs(title = paste("Boxplot of", col), y = col) +
    theme_minimal()
  print(p) # Print each boxplot
}
```

```{r outlier scatter plot matrix}
# Compute summary statistics for the outliers
summary(outlier_df)
```

```{r outliers highlighted in scatter plot}
# Example scatter plot for red_mean vs. green_mean, highlighting outliers
ggplot(results_spectral, aes(x = red_mean, y = green_mean)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_point(data = outlier_df, aes(x = red_mean, y = green_mean), color = "red", size = 3) +
  labs(title = "Red Mean vs. Green Mean with Outliers Highlighted", x = "Red Mean", y = "Green Mean") +
  theme_minimal()

```

```{r brightness after outlier removal}
results_no_outliers <- results_spectral %>% 
  {
    z_scores <- scale(select(., -image_url)) # Calculate Z-scores for numeric columns
    outliers <- apply(z_scores, 1, function(row) any(abs(row) > 3)) # Identify outliers
    .[!outliers, ] # Return rows without outliers
  }
ggplot(results_no_outliers, aes(x = brightness)) + 
  geom_histogram(binwidth = 10, fill = "blue", color = "black") +
  labs(title = "Distribution of Brightness", x = "Brightness", y = "Frequency")

```

```{r dist of color channels after outlier removal}
ggplot(results_no_outliers, aes(x = red_mean)) + 
  geom_histogram(binwidth = 10, fill = "red", color = "black") +
  labs(title = "Distribution of Red Mean", x = "Red Mean", y = "Frequency")

ggplot(results_no_outliers, aes(x = green_mean)) + 
  geom_histogram(binwidth = 10, fill = "green", color = "black") +
  labs(title = "Distribution of Green Mean", x = "Green Mean", y = "Frequency")

ggplot(results_no_outliers, aes(x = blue_mean)) + 
  geom_histogram(binwidth = 10, fill = "blue", color = "black") +
  labs(title = "Distribution of Blue Mean", x = "Blue Mean", y = "Frequency")
```

```{r dist of chromatic coordinates after outlier removal}
# Scatter plot of GCC vs. brightness
ggplot(results_no_outliers, aes(x = brightness, y = GCC)) +
  geom_point(alpha = 0.5, color = "green") +
  labs(title = "Scatter Plot of Brightness vs. GCC", x = "Brightness", y = "GCC")

# Scatter plot of BCC vs. brightness
ggplot(results_no_outliers, aes(x = brightness, y = BCC)) +
  geom_point(alpha = 0.5, color = "blue") +
  labs(title = "Scatter Plot of Brightness vs. BCC", x = "Brightness", y = "BCC")

# Scatter plot of RCC vs. brightness
ggplot(results_no_outliers, aes(x = brightness, y = RCC)) +
  geom_point(alpha = 0.5, color = "red") +
  labs(title = "Scatter Plot of Brightness vs. RCC", x = "Brightness", y = "RCC")

```

```{r individual spectral bands without outliers}
# Scatter plot for Red (x) vs. Blue (y)
ggplot(results_no_outliers, aes(x = red_mean, y = blue_mean)) +
  geom_point(alpha = 0.6, color = "blue") +
  labs(title = "Scatter Plot of Red Mean vs. Blue Mean", x = "Red Mean (x)", y = "Blue Mean (y)")

# Scatter plot for Red (x) vs. Green (z)
ggplot(results_no_outliers, aes(x = red_mean, y = green_mean)) +
  geom_point(alpha = 0.6, color = "green") +
  labs(title = "Scatter Plot of Red Mean vs. Green Mean", x = "Red Mean (x)", y = "Green Mean (z)")

# Scatter plot for Blue (y) vs. Green (z)
ggplot(results_no_outliers, aes(x = blue_mean, y = green_mean)) +
  geom_point(alpha = 0.6, color = "cyan") +
  labs(title = "Scatter Plot of Blue Mean (y) vs. Green Mean (z)", x = "Blue Mean (y)", y = "Green Mean (z)")
```

```{r color magnitude no outliers}
# Histogram of the color magnitude
ggplot(results_no_outliers, aes(x = color_magnitude)) +
  geom_histogram(binwidth = 10, fill = "purple", color = "black") +
  labs(title = "Distribution of Color Magnitude", x = "Color Magnitude", y = "Frequency")
```

# LASSO Experiment

```{r load merged data}
merged <- read_csv("merged_data.csv")
```

```{r process images again}
# Load necessary libraries
library(imager)

# Define the number of samples you want
sample_size <- 200  # Adjust this as needed

# Randomly sample a subset of rows from the merged dataset
set.seed(123)  # For reproducibility
sampled_data <- merged[sample(1:nrow(merged), sample_size), ]

# Extract the 6 columns with image URLs
image_url_columns <- c("north_photo_url", "east_photo_url", "south_photo_url", 
                       "west_photo_url", "upward_photo_url", "downward_photo_url")

# Flatten the URLs into a single vector
image_urls <- unlist(sampled_data[ , image_url_columns])

# Remove any NA values (in case some URLs are missing)
image_urls <- na.omit(image_urls)

# Updated function to handle errors gracefully
download_and_process_image <- function(image_url) {
  temp_file <- tempfile(fileext = ".jpg")
  
  # Try to download the image and catch errors
  tryCatch({
    download.file(image_url, temp_file, mode = "wb")
    image <- load.image(temp_file)
    
    # Check if the image loaded correctly
    if (is.null(image)) {
      return(NULL)
    }
    
    # Ensure image values are properly scaled
    if (max(image) <= 1) {
      image <- image * 255
    }
    
    # Greyscale conversion using the weighted method
    greyscale_image <- 0.299 * image[,,1] + 0.587 * image[,,2] + 0.114 * image[,,3]
    brightness <- mean(greyscale_image)
    
    red_mean <- mean(image[,,1])
    green_mean <- mean(image[,,2])
    blue_mean <- mean(image[,,3])
    
    # Return the metrics and the original image URL
    return(list(image_url = image_url, brightness = brightness, 
                red_mean = red_mean, green_mean = green_mean, blue_mean = blue_mean))
  }, error = function(e) {
    message("Error processing image from URL: ", image_url)
    return(NULL)
  })
}

# Re-run the image processing
results <- lapply(image_urls, download_and_process_image)
results <- results[!sapply(results, is.null)]  # Remove NULL results

# Convert the results to a data frame
results_df <- do.call(rbind, lapply(results, as.data.frame))

# Ensure the "image_url" column exists in results_df for merging
if (!"image_url" %in% names(results_df)) {
  stop("The 'image_url' column is missing in results_df.")
}

# Find the column in image_url_columns with the most non-NA values for merging
image_url_column <- image_url_columns[which.max(colSums(!is.na(sampled_data[ , image_url_columns])))]

# Merge results_df with sampled_data to add surface conditions and other attributes
merged_results_df <- merge(sampled_data, results_df, 
                           by.x = image_url_column, 
                           by.y = "image_url", all.x = TRUE)

# Check if the merged results are correct
print(head(merged_results_df))
```

```{r adding chromatic coords}
# Calculate GCC, BCC, and RCC
merged_results_df$GCC <- merged_results_df$green_mean / (merged_results_df$red_mean + merged_results_df$green_mean + merged_results_df$blue_mean)
merged_results_df$BCC <- merged_results_df$blue_mean / (merged_results_df$red_mean + merged_results_df$green_mean + merged_results_df$blue_mean)
merged_results_df$RCC <- merged_results_df$red_mean / (merged_results_df$red_mean + merged_results_df$green_mean + merged_results_df$blue_mean)

summary(merged_results_df)
```

```{r adding color magnitude}
# Calculate the Euclidean norm
merged_results_df$color_magnitude <- sqrt(merged_results_df$red_mean^2 + merged_results_df$blue_mean^2 + merged_results_df$green_mean^2)

# Summary of the new color magnitude column
summary(merged_results_df$color_magnitude)
```

```{r saving merged_results_df}
write_csv(merged_results_df, "dig_att_200.csv")
```

```{r lasso regression}
# Load necessary library
library(glmnet)

# Prepare the data for LASSO regression
# Select numeric predictor columns (excluding image URLs and target variables)
predictor_columns <- c("brightness", "red_mean", "green_mean", "blue_mean")
x <- as.matrix(merged_results_df[, predictor_columns])

# Target variables
y_snow <- merged_results_df$snow_ice
y_leaves <- merged_results_df$leaves_on_trees

# Ensure target variables are numeric (if they are not binary 0/1, consider converting them)
y_snow <- as.numeric(y_snow)
y_leaves <- as.numeric(y_leaves)

# Fit the LASSO model for snow_ice
lasso_snow <- glmnet(x, y_snow, alpha = 1, family = "binomial")

# Fit the LASSO model for leaves_on_trees
lasso_leaves <- glmnet(x, y_leaves, alpha = 1, family = "binomial")

# Print a summary of the models
print("Summary of LASSO model for snow_ice:")
print(summary(lasso_snow))

print("Summary of LASSO model for leaves_on_trees:")
print(summary(lasso_leaves))

# Plot the LASSO paths
par(mfrow = c(1, 2))  # Set up the plotting area to display both plots side by side
plot(lasso_snow, xvar = "lambda", label = TRUE, main = "LASSO Path for Snow Ice")
plot(lasso_leaves, xvar = "lambda", label = TRUE, main = "LASSO Path for Leaves on Trees")

```

The plots show how the coefficients shrink as the penalty (`lambda`) increases.

```{r selecting lambda}
# Select the smallest lambda value from the lambda sequence
lambda_snow <- min(lasso_snow$lambda)
lambda_leaves <- min(lasso_leaves$lambda)
```

```{r predictions and confusion matrices}
# Load necessary libraries
library(glmnet)
library(caret)  # For creating confusion matrices

# Make predictions for snow_ice
snow_ice_probabilities <- predict(lasso_snow, x, s = lambda_snow, type = "response")
snow_ice_predictions <- ifelse(snow_ice_probabilities > 0.5, 1, 0)

# Make predictions for leaves_on_trees
leaves_on_trees_probabilities <- predict(lasso_leaves, x, s = lambda_leaves, type = "response")
leaves_on_trees_predictions <- ifelse(leaves_on_trees_probabilities > 0.5, 1, 0)

# Create confusion matrices
confusion_matrix_snow <- confusionMatrix(as.factor(snow_ice_predictions), as.factor(y_snow))
confusion_matrix_leaves <- confusionMatrix(as.factor(leaves_on_trees_predictions), as.factor(y_leaves))

# Print the confusion matrices
print("Confusion Matrix for Snow Ice:")
print(confusion_matrix_snow)

print("Confusion Matrix for Leaves on Trees:")
print(confusion_matrix_leaves)

```

```{r CV}
# Perform cross-validation for snow_ice
cv_lasso_snow <- cv.glmnet(x, y_snow, alpha = 1, family = "binomial")
optimal_lambda_snow <- cv_lasso_snow$lambda.min

# Perform cross-validation for leaves_on_trees
cv_lasso_leaves <- cv.glmnet(x, y_leaves, alpha = 1, family = "binomial")
optimal_lambda_leaves <- cv_lasso_leaves$lambda.min

# Make predictions using the optimal lambda values
snow_ice_probabilities <- predict(lasso_snow, x, s = optimal_lambda_snow, type = "response")
snow_ice_predictions <- ifelse(snow_ice_probabilities > 0.5, 1, 0)

leaves_on_trees_probabilities <- predict(lasso_leaves, x, s = optimal_lambda_leaves, type = "response")
leaves_on_trees_predictions <- ifelse(leaves_on_trees_probabilities > 0.5, 1, 0)

# Create confusion matrices
confusion_matrix_snow <- confusionMatrix(as.factor(snow_ice_predictions), as.factor(y_snow))
confusion_matrix_leaves <- confusionMatrix(as.factor(leaves_on_trees_predictions), as.factor(y_leaves))

# Print the confusion matrices
print("Confusion Matrix for Snow Ice with Optimal Lambda:")
print(confusion_matrix_snow)

print("Confusion Matrix for Leaves on Trees with Optimal Lambda:")
print(confusion_matrix_leaves)

```

```{r trying diff thresholds}
# Adjust the threshold for predictions (e.g., 0.4 instead of 0.5)
new_threshold <- 0.4
snow_ice_predictions <- ifelse(snow_ice_probabilities > new_threshold, 1, 0)
leaves_on_trees_predictions <- ifelse(leaves_on_trees_probabilities > new_threshold, 1, 0)

# Re-evaluate the confusion matrices with the new threshold
confusion_matrix_snow <- confusionMatrix(as.factor(snow_ice_predictions), as.factor(y_snow))
confusion_matrix_leaves <- confusionMatrix(as.factor(leaves_on_trees_predictions), as.factor(y_leaves))

print("Confusion Matrix for Snow Ice with Adjusted Threshold:")
print(confusion_matrix_snow)

print("Confusion Matrix for Leaves on Trees with Adjusted Threshold:")
print(confusion_matrix_leaves)

```

# CCA

```{r install cca package}
install.packages("CCA")
```

```{r prepare data for cca}
# Load the CCA package
library(CCA)

# Prepare the sets of variables for CCA
# Set 1: Digital attributes
X <- as.matrix(merged_results_df[, c("brightness", "red_mean", "green_mean", "blue_mean")])

# Set 2: Target variables (convert to numeric if necessary)
Y <- as.matrix(merged_results_df[, c("snow_ice", "leaves_on_trees")])
Y <- apply(Y, 2, as.numeric)  # Ensure Y is numeric

```

```{r run cca}
# Perform CCA
cca_result <- cancor(X, Y)

# Print the canonical correlations
print("Canonical Correlations:")
print(cca_result$cor)

# Display the canonical coefficients for X and Y
print("Canonical Coefficients for X (Digital Attributes):")
print(cca_result$xcoef)

print("Canonical Coefficients for Y (Target Variables):")
print(cca_result$ycoef)

```

1.  **Canonical Correlations**:

    -   The canonical correlations you obtained are `0.2196` and `0.1660`. These values indicate the strength of the relationships between the pairs of canonical variables derived from the digital attributes and the target variables.

    -   Since both correlations are relatively low (closer to 0 than 1), it suggests that the relationship between the sets of variables (`X` and `Y`) is not very strong.

2.  **Canonical Coefficients for `X` (Digital Attributes)**:

    -   These coefficients indicate how each original digital attribute (brightness, red_mean, and green_mean) contributes to the canonical variables for the digital attribute set.

    -   **Interpretation**:

        -   The values are quite small, suggesting that none of the digital attributes have a strong influence on the canonical variables derived from `X`.

        -   `brightness` has a slightly higher coefficient in the first canonical variable, but it’s still quite small.

3.  **Canonical Coefficients for `Y` (Target Variables)**:

    -   These coefficients show the contribution of each target variable (`snow_ice` and `leaves_on_trees`) to the canonical variables for the target variable set.

    -   **Interpretation**:

        -   `snow_ice` has a stronger negative contribution to the first canonical variable, while `leaves_on_trees` has a smaller positive contribution.

        -   In the second canonical variable, `snow_ice` and `leaves_on_trees` both contribute positively but with moderate coefficients.

### What These Results Indicate

-   **Weak Relationships**: The relatively low canonical correlations suggest that the digital attributes do not have a strong linear relationship with the target variables (`snow_ice` and `leaves_on_trees`).

-   **Potential for Further Analysis**: Since CCA did not reveal strong relationships, you may want to explore other methods or consider that the relationship between your variables may be non-linear or influenced by other factors not captured by the digital attributes.
